{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfu/miniconda3/envs/dp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import os\n",
    "import yaml \n",
    "from pathlib import Path\n",
    "from dp_gs.util.args import ExperimentConfig\n",
    "from transformers import AutoProcessor\n",
    "from dp_gs.policy.model import Dinov2DiscretePolicy\n",
    "from dp_gs.dataset.utils import default_vision_transform, aug_vision_transform\n",
    "from dp_gs.dataset.image_dataset import SequenceDataset, VideoSampler, CollateFunction\n",
    "from dp_gs.dataset.image_dataset_sim import SequenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt_folder = \"/shared/projects/icrl/dp_outputs/250208_1027\"\n",
    "\n",
    "ckpt1_path = \"/shared/projects/icrl/dp_outputs/250208_1027/checkpoint_50.pt\"\n",
    "ckpt2_path = \"/shared/projects/icrl/dp_outputs/250208_1027/checkpoint_95.pt\"\n",
    "\n",
    "ckpt1 = torch.load(ckpt1_path, weights_only=True)\n",
    "ckpt2 = torch.load(ckpt2_path, weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The checkpoints have the same keys.\n",
      "Keys with the same values:\n",
      "module.dino.norm.bias\n",
      "module.dino.blocks.6.norm2.weight\n",
      "module.dino.blocks.1.mlp.fc1.bias\n",
      "module.dino.blocks.11.ls2.gamma\n",
      "module.dino.blocks.4.ls2.gamma\n",
      "module.decoder.blocks.1.cross_attention.rope.emb\n",
      "module.dino.patch_embed.proj.weight\n",
      "module.dino.blocks.1.attn.qkv.bias\n",
      "module.dino.blocks.1.norm2.bias\n",
      "module.dino.blocks.3.attn.proj.weight\n",
      "module.dino.blocks.6.mlp.fc2.weight\n",
      "module.dino.blocks.3.norm2.bias\n",
      "module.dino.blocks.10.ls2.gamma\n",
      "module.dino.blocks.5.mlp.fc1.weight\n",
      "module.dino.blocks.5.norm2.bias\n",
      "module.dino.blocks.8.mlp.fc2.bias\n",
      "module.dino.blocks.10.norm2.bias\n",
      "module.dino.blocks.8.attn.proj.weight\n",
      "module.dino.blocks.1.ls1.gamma\n",
      "module.decoder.blocks.3.self_attention.rope.inv_freq\n",
      "module.dino.blocks.1.norm1.bias\n",
      "module.dino.blocks.1.norm2.weight\n",
      "module.dino.blocks.6.norm2.bias\n",
      "module.dino.blocks.5.attn.qkv.weight\n",
      "module.dino.blocks.6.ls1.gamma\n",
      "module.dino.blocks.9.attn.proj.bias\n",
      "module.dino.blocks.3.attn.qkv.bias\n",
      "module.dino.blocks.11.attn.qkv.bias\n",
      "module.dino.blocks.10.ls1.gamma\n",
      "module.dino.blocks.3.ls2.gamma\n",
      "module.decoder.blocks.5.cross_attention.rope.emb\n",
      "module.dino.blocks.8.mlp.fc2.weight\n",
      "module.dino.blocks.2.ls2.gamma\n",
      "module.dino.blocks.2.ls1.gamma\n",
      "module.dino.blocks.11.mlp.fc1.weight\n",
      "module.dino.blocks.8.norm1.weight\n",
      "module.dino.blocks.8.mlp.fc1.weight\n",
      "module.dino.blocks.11.attn.proj.weight\n",
      "module.dino.blocks.6.attn.proj.weight\n",
      "module.dino.blocks.9.mlp.fc1.bias\n",
      "module.dino.blocks.9.ls1.gamma\n",
      "module.dino.blocks.1.norm1.weight\n",
      "module.dino.blocks.5.norm1.weight\n",
      "module.dino.blocks.10.attn.qkv.bias\n",
      "module.dino.blocks.3.norm2.weight\n",
      "module.dino.blocks.7.ls1.gamma\n",
      "module.dino.blocks.7.norm1.bias\n",
      "module.dino.blocks.5.norm1.bias\n",
      "module.dino.blocks.8.ls2.gamma\n",
      "module.dino.blocks.5.mlp.fc1.bias\n",
      "module.dino.blocks.8.norm1.bias\n",
      "module.dino.blocks.6.ls2.gamma\n",
      "module.decoder.blocks.1.cross_attention.rope.inv_freq\n",
      "module.dino.blocks.6.mlp.fc1.bias\n",
      "module.dino.blocks.4.attn.proj.weight\n",
      "module.dino.blocks.8.norm2.bias\n",
      "module.dino.blocks.5.mlp.fc2.bias\n",
      "module.dino.blocks.9.norm1.bias\n",
      "module.dino.blocks.9.norm2.bias\n",
      "module.decoder.blocks.2.cross_attention.rope.inv_freq\n",
      "module.dino.blocks.1.mlp.fc1.weight\n",
      "module.dino.blocks.7.norm2.weight\n",
      "module.dino.blocks.2.norm1.bias\n",
      "module.decoder.blocks.2.self_attention.rope.inv_freq\n",
      "module.dino.blocks.6.mlp.fc1.weight\n",
      "module.decoder.blocks.4.self_attention.rope.emb\n",
      "module.dino.blocks.0.ls1.gamma\n",
      "module.dino.blocks.7.attn.qkv.weight\n",
      "module.decoder.blocks.2.self_attention.rope.emb\n",
      "module.decoder.blocks.2.cross_attention.rope.emb\n",
      "module.dino.norm.weight\n",
      "module.dino.blocks.4.mlp.fc2.weight\n",
      "module.dino.blocks.5.ls2.gamma\n",
      "module.dino.blocks.2.norm1.weight\n",
      "module.dino.blocks.8.norm2.weight\n",
      "module.decoder.blocks.5.self_attention.rope.emb\n",
      "module.proprio_proj.weight\n",
      "module.dino.blocks.5.attn.proj.bias\n",
      "module.dino.blocks.11.mlp.fc2.weight\n",
      "module.dino.blocks.9.attn.qkv.bias\n",
      "module.dino.blocks.4.attn.qkv.bias\n",
      "module.decoder.blocks.0.cross_attention.rope.inv_freq\n",
      "module.dino.blocks.10.attn.qkv.weight\n",
      "module.dino.blocks.10.mlp.fc2.weight\n",
      "module.dino.patch_embed.proj.bias\n",
      "module.dino.blocks.3.norm1.weight\n",
      "module.dino.blocks.1.attn.proj.weight\n",
      "module.dino.blocks.0.mlp.fc2.bias\n",
      "module.dino.blocks.3.attn.proj.bias\n",
      "module.dino.blocks.0.ls2.gamma\n",
      "module.dino.blocks.11.ls1.gamma\n",
      "module.dino.blocks.9.attn.qkv.weight\n",
      "module.dino.blocks.10.attn.proj.weight\n",
      "module.dino.blocks.4.attn.qkv.weight\n",
      "module.dino.blocks.0.norm2.bias\n",
      "module.dino.blocks.11.attn.proj.bias\n",
      "module.dino.blocks.7.norm2.bias\n",
      "module.decoder.blocks.5.cross_attention.rope.inv_freq\n",
      "module.dino.blocks.2.norm2.weight\n",
      "module.dino.blocks.9.norm2.weight\n",
      "module.dino.blocks.5.mlp.fc2.weight\n",
      "module.dino.blocks.11.norm2.weight\n",
      "module.dino.blocks.0.norm2.weight\n",
      "module.dino.blocks.8.mlp.fc1.bias\n",
      "module.dino.blocks.2.mlp.fc2.weight\n",
      "module.dino.blocks.4.ls1.gamma\n",
      "module.dino.blocks.11.norm1.weight\n",
      "module.dino.blocks.11.attn.qkv.weight\n",
      "module.dino.blocks.0.attn.proj.weight\n",
      "module.dino.cls_token\n",
      "module.dino.blocks.5.norm2.weight\n",
      "module.dino.blocks.4.norm1.weight\n",
      "module.dino.blocks.4.norm1.bias\n",
      "module.dino.blocks.7.ls2.gamma\n",
      "module.dino.blocks.10.norm2.weight\n",
      "module.dino.blocks.7.mlp.fc2.bias\n",
      "module.decoder.blocks.1.self_attention.rope.emb\n",
      "module.dino.blocks.7.mlp.fc2.weight\n",
      "module.dino.blocks.10.mlp.fc1.bias\n",
      "module.dino.blocks.1.attn.qkv.weight\n",
      "module.dino.blocks.8.attn.qkv.weight\n",
      "module.dino.blocks.1.ls2.gamma\n",
      "module.decoder.blocks.0.self_attention.rope.inv_freq\n",
      "module.dino.blocks.6.attn.proj.bias\n",
      "module.dino.blocks.1.mlp.fc2.weight\n",
      "module.dino.blocks.8.ls1.gamma\n",
      "module.dino.blocks.9.mlp.fc2.weight\n",
      "module.dino.blocks.2.attn.proj.bias\n",
      "module.decoder.blocks.3.self_attention.rope.emb\n",
      "module.dino.blocks.4.norm2.bias\n",
      "module.dino.blocks.4.attn.proj.bias\n",
      "module.dino.blocks.4.mlp.fc1.bias\n",
      "module.dino.blocks.3.mlp.fc1.bias\n",
      "module.dino.blocks.4.norm2.weight\n",
      "module.dino.blocks.10.mlp.fc2.bias\n",
      "module.decoder.blocks.3.cross_attention.rope.emb\n",
      "module.dino.blocks.3.norm1.bias\n",
      "module.dino.blocks.9.ls2.gamma\n",
      "module.dino.blocks.9.attn.proj.weight\n",
      "module.dino.blocks.2.mlp.fc2.bias\n",
      "module.dino.blocks.6.norm1.weight\n",
      "module.dino.blocks.0.mlp.fc1.bias\n",
      "module.dino.blocks.11.mlp.fc2.bias\n",
      "module.dino.blocks.1.mlp.fc2.bias\n",
      "module.dino.blocks.6.attn.qkv.bias\n",
      "module.dino.blocks.4.mlp.fc2.bias\n",
      "module.dino.blocks.2.norm2.bias\n",
      "module.dino.blocks.7.attn.proj.bias\n",
      "module.dino.blocks.9.mlp.fc2.bias\n",
      "module.dino.blocks.2.attn.qkv.bias\n",
      "module.dino.blocks.5.attn.qkv.bias\n",
      "module.dino.blocks.11.norm2.bias\n",
      "module.dino.blocks.10.norm1.bias\n",
      "module.decoder.blocks.4.cross_attention.rope.inv_freq\n",
      "module.dino.blocks.3.mlp.fc2.weight\n",
      "module.decoder.blocks.0.self_attention.rope.emb\n",
      "module.dino.blocks.0.attn.proj.bias\n",
      "module.dino.blocks.0.attn.qkv.bias\n",
      "module.decoder.blocks.3.cross_attention.rope.inv_freq\n",
      "module.dino.blocks.0.mlp.fc2.weight\n",
      "module.dino.blocks.3.attn.qkv.weight\n",
      "module.dino.blocks.7.mlp.fc1.weight\n",
      "module.decoder.blocks.1.self_attention.rope.inv_freq\n",
      "module.dino.blocks.5.attn.proj.weight\n",
      "module.dino.blocks.10.norm1.weight\n",
      "module.dino.blocks.2.attn.qkv.weight\n",
      "module.dino.blocks.4.mlp.fc1.weight\n",
      "module.dino.blocks.1.attn.proj.bias\n",
      "module.dino.blocks.7.mlp.fc1.bias\n",
      "module.dino.blocks.9.norm1.weight\n",
      "module.dino.blocks.11.norm1.bias\n",
      "module.dino.blocks.3.ls1.gamma\n",
      "module.dino.blocks.9.mlp.fc1.weight\n",
      "module.dino.blocks.2.mlp.fc1.weight\n",
      "module.dino.blocks.0.attn.qkv.weight\n",
      "module.dino.blocks.0.norm1.bias\n",
      "module.dino.blocks.5.ls1.gamma\n",
      "module.decoder.blocks.4.cross_attention.rope.emb\n",
      "module.decoder.blocks.0.cross_attention.rope.emb\n",
      "module.dino.blocks.6.attn.qkv.weight\n",
      "module.dino.blocks.7.attn.proj.weight\n",
      "module.decoder.blocks.4.self_attention.rope.inv_freq\n",
      "module.dino.blocks.2.mlp.fc1.bias\n",
      "module.dino.pos_embed\n",
      "module.dino.blocks.2.attn.proj.weight\n",
      "module.dino.blocks.7.norm1.weight\n",
      "module.dino.blocks.7.attn.qkv.bias\n",
      "module.dino.blocks.6.mlp.fc2.bias\n",
      "module.dino.blocks.3.mlp.fc2.bias\n",
      "module.dino.blocks.8.attn.qkv.bias\n",
      "module.dino.blocks.8.attn.proj.bias\n",
      "module.dino.blocks.0.mlp.fc1.weight\n",
      "module.dino.blocks.0.norm1.weight\n",
      "module.decoder.blocks.5.self_attention.rope.inv_freq\n",
      "module.dino.blocks.10.attn.proj.bias\n",
      "module.dino.blocks.10.mlp.fc1.weight\n",
      "module.dino.blocks.11.mlp.fc1.bias\n",
      "module.dino.blocks.3.mlp.fc1.weight\n",
      "module.proprio_proj.bias\n",
      "module.dino.blocks.6.norm1.bias\n",
      "\n",
      "Keys with different values:\n",
      "module.decoder.blocks.5.output.weight\n",
      "module.decoder.blocks.2.self_attention.value.weight\n",
      "module.decoder.blocks.5.layernorm3.bias\n",
      "module.decoder.blocks.0.self_attention.key.weight\n",
      "module.decoder.blocks.3.cross_attention.out.weight\n",
      "module.decoder.blocks.1.layernorm1.bias\n",
      "module.decoder.blocks.4.cross_attention.key.weight\n",
      "module.decoder.blocks.5.layernorm1.weight\n",
      "module.decoder.blocks.1.layernorm1.weight\n",
      "module.decoder.blocks.5.cross_attention.key.weight\n",
      "module.decoder.blocks.5.self_attention.value.weight\n",
      "module.decoder.blocks.2.self_attention.key.weight\n",
      "module.decoder.blocks.3.output.bias\n",
      "module.decoder.blocks.4.intermediate.bias\n",
      "module.decoder.blocks.5.layernorm2.bias\n",
      "module.decoder.blocks.1.self_attention.query.weight\n",
      "module.decoder.blocks.4.cross_attention.value.weight\n",
      "module.decoder.blocks.2.intermediate.bias\n",
      "module.decoder.blocks.4.self_attention.key.bias\n",
      "module.decoder.blocks.0.self_attention.value.bias\n",
      "module.decoder.blocks.3.self_attention.query.weight\n",
      "module.decoder.blocks.0.intermediate.weight\n",
      "module.decoder.blocks.2.cross_attention.query.bias\n",
      "module.decoder.blocks.0.cross_attention.query.bias\n",
      "module.decoder.blocks.2.cross_attention.out.weight\n",
      "module.decoder.blocks.4.cross_attention.key.bias\n",
      "module.decoder.blocks.3.layernorm1.weight\n",
      "module.decoder.blocks.3.cross_attention.value.weight\n",
      "module.decoder.blocks.0.cross_attention.value.bias\n",
      "module.decoder.blocks.2.cross_attention.out.bias\n",
      "module.decoder.blocks.0.self_attention.key.bias\n",
      "module.decoder.blocks.2.layernorm2.bias\n",
      "module.decoder.blocks.4.cross_attention.out.weight\n",
      "module.decoder.blocks.2.self_attention.key.bias\n",
      "module.decoder.blocks.2.layernorm3.bias\n",
      "module.decoder.blocks.3.layernorm3.bias\n",
      "module.decoder.blocks.1.self_attention.key.bias\n",
      "module.decoder.blocks.2.layernorm1.weight\n",
      "module.decoder.blocks.1.self_attention.query.bias\n",
      "module.decoder.blocks.2.output.weight\n",
      "module.decoder.blocks.3.layernorm2.bias\n",
      "module.decoder.blocks.5.self_attention.key.bias\n",
      "module.decoder.blocks.5.self_attention.value.bias\n",
      "module.decoder.output_proj.weight\n",
      "module.decoder.blocks.4.intermediate.weight\n",
      "module.decoder.blocks.4.self_attention.value.weight\n",
      "module.decoder.blocks.5.cross_attention.value.weight\n",
      "module.decoder.blocks.5.cross_attention.key.bias\n",
      "module.decoder.final_ln.weight\n",
      "module.decoder.blocks.3.cross_attention.key.weight\n",
      "module.decoder.output_proj.bias\n",
      "module.decoder.blocks.3.cross_attention.query.weight\n",
      "module.decoder.blocks.1.self_attention.value.bias\n",
      "module.decoder.blocks.2.cross_attention.value.bias\n",
      "module.decoder.blocks.4.self_attention.query.bias\n",
      "module.decoder.blocks.4.layernorm2.bias\n",
      "module.decoder.blocks.5.self_attention.query.bias\n",
      "module.decoder.blocks.5.cross_attention.query.weight\n",
      "module.decoder.blocks.0.self_attention.out.bias\n",
      "module.decoder.blocks.5.self_attention.key.weight\n",
      "module.decoder.blocks.0.layernorm2.weight\n",
      "module.decoder.blocks.4.self_attention.key.weight\n",
      "module.decoder.blocks.1.cross_attention.key.weight\n",
      "module.decoder.blocks.2.layernorm2.weight\n",
      "module.decoder.blocks.4.output.weight\n",
      "module.decoder.blocks.1.cross_attention.out.weight\n",
      "module.decoder.blocks.2.self_attention.out.bias\n",
      "module.decoder.blocks.2.self_attention.query.bias\n",
      "module.decoder.blocks.2.cross_attention.value.weight\n",
      "module.decoder.blocks.5.intermediate.bias\n",
      "module.decoder.blocks.5.intermediate.weight\n",
      "module.decoder.blocks.0.intermediate.bias\n",
      "module.decoder.blocks.0.self_attention.query.weight\n",
      "module.decoder.blocks.1.cross_attention.query.bias\n",
      "module.decoder.blocks.3.cross_attention.key.bias\n",
      "module.decoder.blocks.4.layernorm1.bias\n",
      "module.decoder.blocks.3.layernorm3.weight\n",
      "module.decoder.blocks.4.self_attention.value.bias\n",
      "module.decoder.blocks.2.cross_attention.query.weight\n",
      "module.decoder.blocks.0.cross_attention.key.bias\n",
      "module.decoder.blocks.1.layernorm3.weight\n",
      "module.decoder.final_ln.bias\n",
      "module.decoder.blocks.0.layernorm2.bias\n",
      "module.decoder.blocks.3.intermediate.weight\n",
      "module.decoder.blocks.1.self_attention.value.weight\n",
      "module.decoder.blocks.2.intermediate.weight\n",
      "module.decoder.blocks.4.cross_attention.value.bias\n",
      "module.decoder.blocks.4.layernorm3.weight\n",
      "module.decoder.blocks.4.self_attention.out.bias\n",
      "module.decoder.blocks.2.cross_attention.key.weight\n",
      "module.decoder.blocks.0.layernorm3.bias\n",
      "module.decoder.blocks.1.cross_attention.value.bias\n",
      "module.decoder.blocks.3.self_attention.value.weight\n",
      "module.decoder.blocks.0.cross_attention.query.weight\n",
      "module.decoder.blocks.2.self_attention.value.bias\n",
      "module.decoder.blocks.0.output.weight\n",
      "module.decoder.blocks.0.self_attention.out.weight\n",
      "module.decoder.blocks.3.self_attention.value.bias\n",
      "module.decoder.blocks.1.self_attention.out.weight\n",
      "module.decoder.blocks.4.layernorm1.weight\n",
      "module.decoder.blocks.2.layernorm1.bias\n",
      "module.decoder.blocks.1.self_attention.key.weight\n",
      "module.decoder.blocks.3.self_attention.out.bias\n",
      "module.decoder.blocks.3.layernorm1.bias\n",
      "module.decoder.blocks.5.self_attention.out.weight\n",
      "module.decoder.blocks.1.cross_attention.query.weight\n",
      "module.decoder.blocks.0.layernorm1.bias\n",
      "module.decoder.blocks.1.intermediate.bias\n",
      "module.decoder.blocks.3.output.weight\n",
      "module.decoder.blocks.2.cross_attention.key.bias\n",
      "module.decoder.blocks.1.cross_attention.out.bias\n",
      "module.decoder.blocks.4.layernorm3.bias\n",
      "module.decoder.blocks.3.cross_attention.value.bias\n",
      "module.decoder.blocks.3.layernorm2.weight\n",
      "module.decoder.blocks.5.layernorm2.weight\n",
      "module.decoder.blocks.0.cross_attention.value.weight\n",
      "module.decoder.blocks.2.self_attention.query.weight\n",
      "module.decoder.blocks.0.cross_attention.out.weight\n",
      "module.decoder.blocks.0.self_attention.value.weight\n",
      "module.decoder.blocks.0.self_attention.query.bias\n",
      "module.decoder.blocks.1.cross_attention.value.weight\n",
      "module.decoder.blocks.0.cross_attention.key.weight\n",
      "module.decoder.blocks.5.self_attention.out.bias\n",
      "module.decoder.blocks.5.cross_attention.value.bias\n",
      "module.decoder.blocks.4.self_attention.out.weight\n",
      "module.decoder.blocks.5.cross_attention.query.bias\n",
      "module.decoder.blocks.5.output.bias\n",
      "module.decoder.blocks.0.layernorm3.weight\n",
      "module.decoder.blocks.2.output.bias\n",
      "module.decoder.blocks.4.self_attention.query.weight\n",
      "module.decoder.blocks.5.layernorm1.bias\n",
      "module.decoder.blocks.1.layernorm2.bias\n",
      "module.decoder.blocks.1.cross_attention.key.bias\n",
      "module.decoder.blocks.1.output.bias\n",
      "module.decoder.blocks.1.output.weight\n",
      "module.decoder.blocks.5.cross_attention.out.bias\n",
      "module.decoder.blocks.4.output.bias\n",
      "module.decoder.blocks.5.cross_attention.out.weight\n",
      "module.decoder.blocks.1.intermediate.weight\n",
      "module.decoder.masked_tokens\n",
      "module.decoder.blocks.3.self_attention.query.bias\n",
      "module.decoder.blocks.5.self_attention.query.weight\n",
      "module.decoder.blocks.4.cross_attention.query.bias\n",
      "module.decoder.blocks.3.cross_attention.out.bias\n",
      "module.decoder.blocks.0.layernorm1.weight\n",
      "module.decoder.blocks.3.self_attention.key.bias\n",
      "module.decoder.blocks.3.self_attention.key.weight\n",
      "module.decoder.blocks.2.self_attention.out.weight\n",
      "module.decoder.blocks.0.cross_attention.out.bias\n",
      "module.decoder.blocks.5.layernorm3.weight\n",
      "module.decoder.blocks.2.layernorm3.weight\n",
      "module.decoder.blocks.4.cross_attention.out.bias\n",
      "module.decoder.blocks.3.cross_attention.query.bias\n",
      "module.decoder.blocks.1.self_attention.out.bias\n",
      "module.decoder.blocks.1.layernorm3.bias\n",
      "module.decoder.blocks.3.intermediate.bias\n",
      "module.decoder.blocks.4.cross_attention.query.weight\n",
      "module.decoder.blocks.3.self_attention.out.weight\n",
      "module.decoder.blocks.4.layernorm2.weight\n",
      "module.decoder.blocks.1.layernorm2.weight\n",
      "module.decoder.blocks.0.output.bias\n"
     ]
    }
   ],
   "source": [
    "# Compare the keys of both checkpoints\n",
    "keys_ckpt1 = set(ckpt1.keys())\n",
    "keys_ckpt2 = set(ckpt2.keys())\n",
    "\n",
    "if keys_ckpt1 != keys_ckpt2:\n",
    "    print(\"The checkpoints have different keys.\")\n",
    "else:\n",
    "    print(\"The checkpoints have the same keys.\")\n",
    "\n",
    "# Compare the values of both checkpoints\n",
    "same_values = []\n",
    "different_values = []\n",
    "\n",
    "for key in keys_ckpt1:\n",
    "    if torch.equal(ckpt1[key], ckpt2[key]):\n",
    "        same_values.append(key)\n",
    "    else:\n",
    "        different_values.append(key)\n",
    "\n",
    "print(\"Keys with the same values:\")\n",
    "for key in same_values:\n",
    "    print(key)\n",
    "\n",
    "print(\"\\nKeys with different values:\")\n",
    "for key in different_values:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_yaml_path = os.path.join(model_ckpt_folder, 'run.yaml')\n",
    "args : ExperimentConfig = yaml.load(Path(train_yaml_path).read_text(), Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: time_horizon, min_token, vocab_size, scale, action_dim. \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoProcessor.from_pretrained(\"physical-intelligence/fast\", trust_remote_code=True)\n",
    "extra_kwargs = {\n",
    "    'vocab_size': tokenizer.vocab_size + 1,  # eos token\n",
    "    'num_tokens': args.shared_cfg.num_tokens\n",
    "}\n",
    "model = Dinov2DiscretePolicy(\n",
    "    shared_cfg=args.shared_cfg,\n",
    "    model_cfg=args.model_cfg,\n",
    "    **(extra_kwargs if args.model_cfg.policy_type == \"discrete\" else {})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt2 = {\n",
    "    key.replace(\"module.\", \"\"): value for key, value in ckpt1.items()\n",
    "}\n",
    "model.load_state_dict(ckpt2, strict=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.shared_cfg.s2:\n",
    "    resolution = args.shared_cfg.image_size * 2\n",
    "else:\n",
    "    resolution = args.shared_cfg.image_size\n",
    "base_vision_transform = default_vision_transform(resolution=resolution) # default img_size: Union[int, Tuple[int, int]] = 224,\n",
    "aug_transform = aug_vision_transform(resolution=resolution) # default img_size: Union[int, Tuple[int, int]] = 224,\n",
    "collate_fn = CollateFunction(\n",
    "    args.shared_cfg.seq_length, \n",
    "    tokenizer=tokenizer, \n",
    "    max_token_length=args.shared_cfg.num_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data before balancing:  1570\n",
      "Length of data after balancing:  1304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1304/1304 [00:08<00:00, 162.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action statistics saved to  /shared/projects/icrl/dp_outputs/250208_1027/action_statistics.json\n",
      "using numeric brightness and contrast augmentation\n",
      "contrast range:  [0.8, 1.2]\n",
      "brightness range:  [-0.1, 0.1]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = SequenceDataset(\n",
    "    dataset_config=args.dataset_cfg,\n",
    "    shared_config=args.shared_cfg,\n",
    "    logging_config=args.logging_cfg,\n",
    "    vision_transform=aug_transform,\n",
    "    split=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.vision_transform = base_vision_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dp_gs.util.misc import MultiEpochsDataLoader\n",
    "train_sampler = VideoSampler(\n",
    "    dataset_train, \n",
    "    batch_size=args.shared_cfg.batch_size, \n",
    "    sequence_length=args.shared_cfg.seq_length, \n",
    ")\n",
    "dataloader_train = MultiEpochsDataLoader(\n",
    "    dataset_train,\n",
    "    batch_sampler=train_sampler,\n",
    "    num_workers=args.trainer_cfg.num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=False,\n",
    "    prefetch_factor=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.3391, device='cuda:0', grad_fn=<NllLossBackward0>), 'acc': tensor(0.8774, device='cuda:0')}\n",
      "{'loss': tensor(0.2982, device='cuda:0', grad_fn=<NllLossBackward0>), 'acc': tensor(0.8968, device='cuda:0')}\n",
      "{'loss': tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward0>), 'acc': tensor(0.8592, device='cuda:0')}\n",
      "{'loss': tensor(0.3804, device='cuda:0', grad_fn=<NllLossBackward0>), 'acc': tensor(0.8652, device='cuda:0')}\n",
      "{'loss': tensor(0.1943, device='cuda:0', grad_fn=<NllLossBackward0>), 'acc': tensor(0.9379, device='cuda:0')}\n",
      "{'loss': tensor(0.4400, device='cuda:0', grad_fn=<NllLossBackward0>), 'acc': tensor(0.8326, device='cuda:0')}\n",
      "{'loss': tensor(0.3351, device='cuda:0', grad_fn=<NllLossBackward0>), 'acc': tensor(0.8871, device='cuda:0')}\n",
      "{'loss': tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward0>), 'acc': tensor(0.9184, device='cuda:0')}\n",
      "{'loss': tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward0>), 'acc': tensor(0.9108, device='cuda:0')}\n",
      "{'loss': tensor(0.2823, device='cuda:0', grad_fn=<NllLossBackward0>), 'acc': tensor(0.8922, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "idx = 0 \n",
    "for dataset_item in dataloader_train:\n",
    "    if idx == 10:\n",
    "        break \n",
    "    idx += 1\n",
    "    for k, v in dataset_item.items():\n",
    "        dataset_item[k] = v.to(device, non_blocking=True)\n",
    "    \n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        loss = model(dataset_item)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
